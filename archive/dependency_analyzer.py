#!/usr/bin/env python3
"""
ğŸ” PROJE PHOENIX - FAZ 0: BAÄIMLILIK ANALÄ°ZÄ° VE REQUIREMENTS GENERATOR
ğŸ’ Statik kod analizi ile eksiksiz baÄŸÄ±mlÄ±lÄ±k grafiÄŸi oluÅŸturma

Bu sistem ÅŸunlarÄ± yapar:
1. âœ… TÃ¼m .py dosyalarÄ±nÄ± statik olarak analiz eder
2. âœ… Import ifadelerinden baÄŸÄ±mlÄ±lÄ±k grafiÄŸi Ã§Ä±karÄ±r  
3. âœ… GerÃ§ekten kullanÄ±lan kÃ¼tÃ¼phaneleri belirler
4. âœ… SÄ±fÄ±rdan eksiksiz requirements.txt oluÅŸturur
5. âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±klarÄ± tespit eder
6. âœ… KullanÄ±lmayan import'larÄ± bulur

KULLANIM:
python dependency_analyzer.py --analyze-all --generate-requirements
python dependency_analyzer.py --check-circular --optimize-imports
"""

import ast
import os
import sys
import logging
import importlib
import subprocess
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Set, Any, Optional, Tuple
import argparse
import json
import re
import traceback
from collections import defaultdict, deque
try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False

# Logging iÃ§in logs klasÃ¶rÃ¼nÃ¼ oluÅŸtur
Path("logs").mkdir(exist_ok=True)

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(Path("logs") / "dependency_analysis.log", mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger("DependencyAnalyzer")

class ImportAnalyzer(ast.NodeVisitor):
    """AST tabanlÄ± import analizi sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.imports = set()
        self.from_imports = set()
        self.local_imports = set()
        self.import_details = []
    
    def visit_Import(self, node):
        for alias in node.names:
            module_name = alias.name
            self.imports.add(module_name)
            self.import_details.append({
                "type": "import",
                "module": module_name,
                "alias": alias.asname,
                "line": node.lineno
            })
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node):
        if node.module:
            module_name = node.module
            self.from_imports.add(module_name)
            
            # Local vs external imports
            if module_name.startswith('.') or module_name in ['utils', 'strategies', 'optimization']:
                self.local_imports.add(module_name)
            
            for alias in node.names:
                self.import_details.append({
                    "type": "from_import",
                    "module": module_name,
                    "name": alias.name,
                    "alias": alias.asname,
                    "line": node.lineno
                })
        self.generic_visit(node)

class DependencyAnalyzer:
    """ğŸ’ Proje Phoenix BaÄŸÄ±mlÄ±lÄ±k Analizi Motoru"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        
        # Logs klasÃ¶rÃ¼nÃ¼ oluÅŸtur (yoksa otomatik oluÅŸtur)
        logs_dir = self.project_root / "logs"
        logs_dir.mkdir(exist_ok=True)
        
        # Log dosyasÄ±nÄ± oluÅŸtur (yoksa)
        log_file = logs_dir / "dependency_analysis.log"
        if not log_file.exists():
            log_file.touch()
        
        # Analiz sonuÃ§larÄ±
        self.file_imports = {}
        self.all_external_imports = set()
        self.all_local_imports = set()
        self.dependency_graph = nx.DiGraph() if NETWORKX_AVAILABLE else {}
        
        # Standart kÃ¼tÃ¼phane modÃ¼lleri (bu modÃ¼lleri requirements'a eklemeyelim)
        self.stdlib_modules = {
            'os', 'sys', 'datetime', 'time', 'json', 'csv', 'math', 'random',
            'collections', 'itertools', 'functools', 'operator', 'pathlib',
            'logging', 'argparse', 'subprocess', 'threading', 'multiprocessing',
            'asyncio', 'typing', 'dataclasses', 'enum', 're', 'ast', 'inspect',
            'warnings', 'traceback', 'tempfile', 'shutil', 'glob', 'pickle',
            'sqlite3', 'urllib', 'http', 'email', 'base64', 'hashlib',
            'uuid', 'decimal', 'fractions', 'statistics', 'copy', 'gc'
        }
        
        logger.info("ğŸ” Dependency Analyzer baÅŸlatÄ±ldÄ±")
        logger.info(f"ğŸ“ Proje kÃ¶kÃ¼: {self.project_root.absolute()}")
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ğŸ“„ Tek dosyanÄ±n import analizini yap"""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                source = f.read()
            
            tree = ast.parse(source)
            analyzer = ImportAnalyzer()
            analyzer.visit(tree)
            
            # External imports (non-stdlib, non-local)
            external_imports = set()
            for imp in analyzer.imports.union(analyzer.from_imports):
                root_module = imp.split('.')[0]
                if (root_module not in self.stdlib_modules and 
                    not imp.startswith('.') and 
                    root_module not in ['utils', 'strategies', 'optimization', 'backtesting', 'scripts']):
                    external_imports.add(root_module)
            
            analysis = {
                "file_path": str(file_path.relative_to(self.project_root)),
                "imports": list(analyzer.imports),
                "from_imports": list(analyzer.from_imports),
                "local_imports": list(analyzer.local_imports),
                "external_imports": list(external_imports),
                "import_details": analyzer.import_details,
                "total_imports": len(analyzer.imports) + len(analyzer.from_imports),
                "lines_of_code": len(source.splitlines())
            }
            
            # Dependency graph'a ekle
            if NETWORKX_AVAILABLE:
                file_node = str(file_path.relative_to(self.project_root))
                for imp in analyzer.imports.union(analyzer.from_imports):
                    self.dependency_graph.add_edge(file_node, imp)
            
            # Global setlere ekle
            self.all_external_imports.update(external_imports)
            
            return analysis
            
        except SyntaxError as e:
            logger.error(f"âŒ Syntax error in {file_path}: {e}")
            return {"error": f"Syntax error: {e}", "file_path": str(file_path)}
        except Exception as e:
            logger.error(f"âŒ Error analyzing {file_path}: {e}")
            return {"error": str(e), "file_path": str(file_path)}
    
    def analyze_all_files(self) -> Dict[str, Any]:
        """ğŸ“‚ TÃ¼m Python dosyalarÄ±nÄ± analiz et"""
        
        logger.info("ğŸ“‚ TÃ¼m Python dosyalarÄ± analiz ediliyor...")
        
        # Python dosyalarÄ±nÄ± bul
        python_files = []
        for pattern in ["*.py", "**/*.py"]:
            python_files.extend(self.project_root.glob(pattern))
        
        # __pycache__ ve .git klasÃ¶rlerini filtrele
        python_files = [
            f for f in python_files 
            if "__pycache__" not in str(f) and ".git" not in str(f)
        ]
        
        logger.info(f"ğŸ“Š {len(python_files)} Python dosyasÄ± bulundu")
        
        # Her dosyayÄ± analiz et
        analysis_results = {}
        successful_analyses = 0
        
        for py_file in python_files:
            file_analysis = self.analyze_file(py_file)
            file_key = str(py_file.relative_to(self.project_root))
            analysis_results[file_key] = file_analysis
            
            if "error" not in file_analysis:
                successful_analyses += 1
                self.file_imports[file_key] = file_analysis
        
        logger.info(f"âœ… {successful_analyses}/{len(python_files)} dosya baÅŸarÄ±yla analiz edildi")
        
        return {
            "total_files": len(python_files),
            "successful_analyses": successful_analyses,
            "failed_analyses": len(python_files) - successful_analyses,
            "all_external_imports": sorted(list(self.all_external_imports)),
            "total_external_imports": len(self.all_external_imports),
            "file_analyses": analysis_results
        }
    
    def check_circular_dependencies(self) -> Dict[str, Any]:
        """ğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼"""
        
        logger.info("ğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼...")
        
        if not NETWORKX_AVAILABLE:
            logger.warning("âš ï¸ NetworkX bulunamadÄ±, dÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k analizi atlanÄ±yor")
            return {"error": "NetworkX not available"}
        
        cycles = []
        
        try:
            # Find cycles in dependency graph
            cycle_generator = nx.simple_cycles(self.dependency_graph)
            for cycle in cycle_generator:
                if len(cycle) > 1:  # Self-loops'larÄ± dahil etme
                    cycles.append(cycle)
        
        except Exception as e:
            logger.error(f"âŒ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼ hatasÄ±: {e}")
            return {"error": str(e)}
        
        if cycles:
            logger.warning(f"âš ï¸ {len(cycles)} dÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k tespit edildi")
            for i, cycle in enumerate(cycles):
                logger.warning(f"  DÃ¶ngÃ¼ {i+1}: {' -> '.join(cycle)} -> {cycle[0]}")
        else:
            logger.info("âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k bulunamadÄ±")
        
        return {
            "has_cycles": len(cycles) > 0,
            "cycle_count": len(cycles),
            "cycles": cycles
        }
    
    def detect_unused_imports(self) -> Dict[str, Any]:
        """ğŸ—‘ï¸ KullanÄ±lmayan import'larÄ± tespit et"""
        
        logger.info("ğŸ—‘ï¸ KullanÄ±lmayan importlar tespit ediliyor...")
        
        unused_imports = {}
        
        for file_path, analysis in self.file_imports.items():
            file_unused = []
            
            try:
                # DosyayÄ± tekrar oku
                full_path = self.project_root / file_path
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Import details'leri kontrol et
                for import_detail in analysis.get("import_details", []):
                    if import_detail["type"] == "import":
                        module_name = import_detail["module"]
                        alias = import_detail.get("alias", module_name)
                        
                        # KullanÄ±m kontrolÃ¼ (basit)
                        if alias not in content.split('\n')[import_detail["line"]:]:
                            # Import satÄ±rÄ±ndan sonra kullanÄ±lmÄ±yor
                            import_usage_count = content.count(alias)
                            if import_usage_count <= 1:  # Sadece import satÄ±rÄ±nda geÃ§iyor
                                file_unused.append(import_detail)
            
            except Exception as e:
                logger.debug(f"Unused import detection error for {file_path}: {e}")
            
            if file_unused:
                unused_imports[file_path] = file_unused
        
        total_unused = sum(len(imports) for imports in unused_imports.values())
        
        if total_unused > 0:
            logger.warning(f"âš ï¸ {total_unused} kullanÄ±lmayan import tespit edildi")
        else:
            logger.info("âœ… KullanÄ±lmayan import bulunamadÄ±")
        
        return {
            "total_unused": total_unused,
            "files_with_unused": len(unused_imports),
            "unused_imports": unused_imports
        }
    
    def _get_package_version(self, package_name: str) -> Optional[str]:
        """ğŸ“¦ Paket versiyonunu al"""
        
        try:
            # pip show komutu ile versiyon bilgisi al
            result = subprocess.run(
                [sys.executable, "-m", "pip", "show", package_name],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if line.startswith('Version:'):
                        return line.split(':')[1].strip()
            
            return None
            
        except Exception as e:
            logger.debug(f"Version detection error for {package_name}: {e}")
            return None
    
    def generate_requirements_txt(self, include_versions: bool = True) -> str:
        """ğŸ“‹ Requirements.txt iÃ§eriÄŸi oluÅŸtur"""
        
        logger.info("ğŸ“‹ Requirements.txt oluÅŸturuluyor...")
        
        # Kategorilere ayÄ±r
        categorized_packages = self._categorize_packages()
        
        requirements_lines = [
            "# ========================================================================================",
            "# ğŸš€ PROJE PHOENIX - AUTO-GENERATED REQUIREMENTS",
            "# ğŸ’ Production-Ready Dependencies for Algorithmic Trading Platform",
            "# ========================================================================================",
            ""
        ]
        
        for category, packages in categorized_packages.items():
            if packages:
                requirements_lines.append(f"# {category}")
                requirements_lines.append("# " + "="*60)
                
                for package in sorted(packages):
                    if include_versions:
                        version = self._get_package_version(package)
                        if version:
                            requirements_lines.append(f"{package}>={version}")
                        else:
                            requirements_lines.append(f"{package}  # Version not detected")
                    else:
                        requirements_lines.append(package)
                
                requirements_lines.append("")
        
        return '\n'.join(requirements_lines)
    
    def _categorize_packages(self) -> Dict[str, List[str]]:
        """ğŸ“¦ Paketleri kategorilere ayÄ±r"""
        
        categories = {
            "Core Data Science": [],
            "Machine Learning": [],
            "Trading & Finance": [],
            "Async & Networking": [],
            "Optimization": [],
            "Visualization": [],
            "Testing & Development": [],
            "System Utilities": [],
            "Other": []
        }
        
        # Kategori mapping'i
        category_mapping = {
            "Core Data Science": ["pandas", "numpy", "scipy", "scikit-learn", "sklearn"],
            "Machine Learning": ["torch", "pytorch", "tensorflow", "keras", "xgboost", "lightgbm", "catboost"],
            "Trading & Finance": ["ccxt", "pandas_ta", "ta", "backtrader", "zipline", "quantlib"],
            "Async & Networking": ["aiohttp", "asyncio-mqtt", "websockets", "requests"],
            "Optimization": ["optuna", "hyperopt", "skopt", "scikit-optimize"],
            "Visualization": ["matplotlib", "seaborn", "plotly", "bokeh"],
            "Testing & Development": ["pytest", "unittest", "mock", "coverage", "black", "flake8"],
            "System Utilities": ["psutil", "python-dateutil", "pytz", "pydantic", "python-dotenv"]
        }
        
        # Paketleri kategorilere ata
        categorized = set()
        
        for category, keywords in category_mapping.items():
            for package in self.all_external_imports:
                if any(keyword in package.lower() for keyword in keywords):
                    categories[category].append(package)
                    categorized.add(package)
        
        # Kategorize edilmeyenleri "Other"a ekle
        for package in self.all_external_imports:
            if package not in categorized:
                categories["Other"].append(package)
        
        # BoÅŸ kategorileri kaldÄ±r
        return {k: v for k, v in categories.items() if v}
    
    def optimize_imports(self) -> Dict[str, Any]:
        """ğŸ”§ Import optimizasyon Ã¶nerileri"""
        
        logger.info("ğŸ”§ Import optimizasyon analizi...")
        
        optimizations = {
            "has_optimizations": False,
            "unused_imports": [],
            "duplicate_imports": [],
            "suggestions": []
        }
        
        # Her dosya iÃ§in optimizasyon Ã¶nerileri
        for file_path, file_analysis in self.file_imports.items():
            file_optimizations = self._analyze_file_import_optimizations(file_analysis)
            
            if file_optimizations["has_optimizations"]:
                optimizations["has_optimizations"] = True
                optimizations["suggestions"].append({
                    "file": file_path,
                    "optimizations": file_optimizations
                })
        
        return optimizations
    
    def _analyze_file_import_optimizations(self, file_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“„ Dosya bazÄ±nda import optimizasyon analizi"""
        
        optimizations = {
            "has_optimizations": False,
            "unused_imports": [],
            "duplicate_imports": [],
            "suggestions": []
        }
        
        # Import details'leri analiz et
        import_details = file_analysis.get("import_details", [])
        
        # Duplicate import detection (basit)
        seen_imports = set()
        for imp in import_details:
            import_key = f"{imp['type']}:{imp['module']}"
            if import_key in seen_imports:
                optimizations["duplicate_imports"].append(imp)
                optimizations["has_optimizations"] = True
            seen_imports.add(import_key)
        
        return optimizations
    
    def save_analysis_results(self, analysis_results: Dict[str, Any]) -> None:
        """ğŸ’¾ Analiz sonuÃ§larÄ±nÄ± kaydet"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON raporu
        json_file = self.project_root / "logs" / f"dependency_analysis_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ Analiz sonuÃ§larÄ± kaydedildi: {json_file}")
        
        # Requirements.txt
        requirements_content = self.generate_requirements_txt()
        requirements_file = self.project_root / "requirements.txt"
        
        # Backup oluÅŸtur
        if requirements_file.exists():
            backup_file = self.project_root / f"requirements.txt.backup_{timestamp}"
            requirements_file.rename(backup_file)
            logger.info(f"ğŸ’¾ Eski requirements.txt backup: {backup_file}")
        
        with open(requirements_file, 'w', encoding='utf-8') as f:
            f.write(requirements_content)
        
        logger.info(f"âœ… Yeni requirements.txt oluÅŸturuldu: {requirements_file}")
        
        # Dependency graph (GraphML format)
        if NETWORKX_AVAILABLE and self.dependency_graph.nodes():
            graph_file = self.project_root / "logs" / f"dependency_graph_{timestamp}.graphml"
            nx.write_graphml(self.dependency_graph, graph_file)
            logger.info(f"ğŸ“ˆ Dependency graph kaydedildi: {graph_file}")
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ğŸ”¬ KapsamlÄ± baÄŸÄ±mlÄ±lÄ±k analizi"""
        
        logger.info("ğŸ”¬ KAPSAMLI BAÄIMLILIK ANALÄ°ZÄ° BAÅLIYOR...")
        
        analysis_start = datetime.now(timezone.utc)
        
        # Ana analiz
        main_analysis = self.analyze_all_files()
        
        # DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼
        circular_analysis = self.check_circular_dependencies()
        
        # KullanÄ±lmayan import'lar
        unused_analysis = self.detect_unused_imports()
        
        # Import optimizasyonlarÄ±
        optimization_analysis = self.optimize_imports()
        
        analysis_end = datetime.now(timezone.utc)
        duration = (analysis_end - analysis_start).total_seconds()
        
        # Comprehensive sonuÃ§
        comprehensive_results = {
            "timestamp": analysis_end.isoformat(),
            "duration_seconds": duration,
            "project_root": str(self.project_root.absolute()),
            "analysis_summary": {
                "total_files_analyzed": main_analysis["successful_analyses"],
                "total_external_imports": len(self.all_external_imports),
                "has_circular_dependencies": circular_analysis.get("has_cycles", False),
                "total_unused_imports": unused_analysis.get("total_unused", 0),
                "optimization_opportunities": optimization_analysis.get("has_optimizations", False)
            },
            "main_analysis": main_analysis,
            "circular_dependencies": circular_analysis,
            "unused_imports": unused_analysis,
            "import_optimizations": optimization_analysis
        }
        
        # SonuÃ§larÄ± kaydet
        self.save_analysis_results(comprehensive_results)
        
        # Konsol Ã¶zeti
        logger.info("="*80)
        logger.info("ğŸ”¬ BAÄIMLILIK ANALÄ°ZÄ° RAPORU")
        logger.info("="*80)
        logger.info(f"ğŸ“Š Analiz edilen dosya: {main_analysis['successful_analyses']}")
        logger.info(f"ğŸ“¦ External import: {len(self.all_external_imports)}")
        logger.info(f"ğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k: {'VAR' if circular_analysis.get('has_cycles', False) else 'YOK'}")
        logger.info(f"ğŸ—‘ï¸ KullanÄ±lmayan import: {unused_analysis.get('total_unused', 0)}")
        logger.info(f"ğŸ”§ Optimizasyon fÄ±rsatÄ±: {'VAR' if optimization_analysis.get('has_optimizations', False) else 'YOK'}")
        logger.info(f"â±ï¸ Analiz sÃ¼resi: {duration:.2f} saniye")
        logger.info("="*80)
        
        return comprehensive_results


def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    
    parser = argparse.ArgumentParser(
        description="Phoenix Dependency Analyzer - Comprehensive Project Analysis",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
KullanÄ±m Ã–rnekleri:
  python dependency_analyzer.py --analyze-all                    # Tam analiz
  python dependency_analyzer.py --generate-requirements          # Requirements.txt oluÅŸtur
  python dependency_analyzer.py --check-circular                 # DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼
  python dependency_analyzer.py --optimize-imports               # Import optimizasyonu
  python dependency_analyzer.py --full-analysis                  # KapsamlÄ± analiz
        """
    )
    
    parser.add_argument('--analyze-all', action='store_true', help='TÃ¼m projeyi analiz et')
    parser.add_argument('--generate-requirements', action='store_true', help='Requirements.txt oluÅŸtur')
    parser.add_argument('--check-circular', action='store_true', help='DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼')
    parser.add_argument('--optimize-imports', action='store_true', help='Import optimizasyonu')
    parser.add_argument('--full-analysis', action='store_true', help='KapsamlÄ± analiz (hepsi)')
    parser.add_argument('--include-versions', action='store_true', default=True, help='Requirements.txt\'ye versiyon bilgisi ekle')
    parser.add_argument('--project-root', default='.', help='Proje kÃ¶k dizini')
    
    args = parser.parse_args()
    
    # Analyzer oluÅŸtur
    analyzer = DependencyAnalyzer(project_root=args.project_root)
    
    try:
        if args.full_analysis or (not any([args.analyze_all, args.generate_requirements, 
                                          args.check_circular, args.optimize_imports])):
            # VarsayÄ±lan: kapsamlÄ± analiz
            results = analyzer.run_comprehensive_analysis()
            
        else:
            # Belirli analizler
            if args.analyze_all:
                results = analyzer.analyze_all_files()
                logger.info(f"âœ… {results['successful_analyses']} dosya analiz edildi")
            
            if args.check_circular:
                circular_results = analyzer.check_circular_dependencies()
                if circular_results.get("has_cycles"):
                    logger.warning(f"âš ï¸ {circular_results['cycle_count']} dÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k bulundu")
                else:
                    logger.info("âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k yok")
            
            if args.optimize_imports:
                opt_results = analyzer.optimize_imports()
                if opt_results.get("has_optimizations"):
                    logger.info("ğŸ”§ Import optimizasyon fÄ±rsatlarÄ± bulundu")
                else:
                    logger.info("âœ… Import'lar optimize")
            
            if args.generate_requirements:
                # Ä°lk Ã¶nce analiz yap
                analyzer.analyze_all_files()
                # Sonra requirements oluÅŸtur
                req_content = analyzer.generate_requirements_txt(args.include_versions)
                req_file = analyzer.project_root / "requirements.txt"
                with open(req_file, 'w') as f:
                    f.write(req_content)
                logger.info(f"âœ… Requirements.txt oluÅŸturuldu: {req_file}")
        
        logger.info("ğŸ‰ BaÄŸÄ±mlÄ±lÄ±k analizi tamamlandÄ±!")
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Analiz kullanÄ±cÄ± tarafÄ±ndan durduruldu")
        sys.exit(130)
    except Exception as e:
        logger.error(f"âŒ Beklenmedik hata: {e}")
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()