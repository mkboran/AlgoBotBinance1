#!/usr/bin/env python3
"""
ğŸ” PROJE PHOENIX - FAZ 0: BAÄIMLILIK ANALÄ°ZÄ° VE REQUIREMENTS GENERATOR
ğŸ’ Statik kod analizi ile eksiksiz baÄŸÄ±mlÄ±lÄ±k grafiÄŸi oluÅŸturma

Bu sistem ÅŸunlarÄ± yapar:
1. âœ… TÃ¼m .py dosyalarÄ±nÄ± statik olarak analiz eder
2. âœ… Import ifadelerinden baÄŸÄ±mlÄ±lÄ±k grafiÄŸi Ã§Ä±karÄ±r  
3. âœ… GerÃ§ekten kullanÄ±lan kÃ¼tÃ¼phaneleri belirler
4. âœ… SÄ±fÄ±rdan eksiksiz requirements.txt oluÅŸturur
5. âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±klarÄ± tespit eder
6. âœ… KullanÄ±lmayan import'larÄ± bulur

KULLANIM:
python dependency_analyzer.py --analyze-all --generate-requirements
python dependency_analyzer.py --check-circular --optimize-imports
"""

import ast
import os
import sys
import logging
import importlib
import subprocess
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Set, Any, Optional, Tuple
import argparse
import json
import re
from collections import defaultdict, deque
import networkx as nx

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(Path("logs") / "dependency_analysis.log", mode='w', encoding='utf-8')
    ]
)
logger = logging.getLogger("DependencyAnalyzer")

class ImportAnalyzer(ast.NodeVisitor):
    """AST tabanlÄ± import analizi sÄ±nÄ±fÄ±"""
    
    def __init__(self):
        self.imports = set()
        self.from_imports = set()
        self.local_imports = set()
        self.import_details = []
    
    def visit_Import(self, node):
        for alias in node.names:
            module_name = alias.name
            self.imports.add(module_name)
            self.import_details.append({
                "type": "import",
                "module": module_name,
                "alias": alias.asname,
                "line": node.lineno
            })
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node):
        if node.module:
            module_name = node.module
            self.from_imports.add(module_name)
            
            # Local vs external imports
            if module_name.startswith('.') or module_name in ['utils', 'strategies', 'optimization']:
                self.local_imports.add(module_name)
            
            for alias in node.names:
                self.import_details.append({
                    "type": "from_import",
                    "module": module_name,
                    "name": alias.name,
                    "alias": alias.asname,
                    "line": node.lineno
                })
        self.generic_visit(node)

class DependencyAnalyzer:
    """ğŸ’ Proje Phoenix BaÄŸÄ±mlÄ±lÄ±k Analizi Motoru"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        
        # Logs klasÃ¶rÃ¼nÃ¼ oluÅŸtur
        (self.project_root / "logs").mkdir(exist_ok=True)
        
        # Analiz sonuÃ§larÄ±
        self.file_dependencies = {}
        self.all_imports = set()
        self.external_packages = set()
        self.local_modules = set()
        self.dependency_graph = nx.DiGraph()
        
        # Python built-in modules
        self.builtin_modules = set(sys.builtin_module_names)
        self.standard_library = self._get_standard_library_modules()
        
        # Proje spesifik modÃ¼ller
        self.project_modules = {'utils', 'strategies', 'optimization', 'scripts'}
        
        logger.info("ğŸ” Dependency Analyzer baÅŸlatÄ±ldÄ±")
        logger.info(f"ğŸ“ Proje kÃ¶kÃ¼: {self.project_root.absolute()}")
    
    def _get_standard_library_modules(self) -> Set[str]:
        """Python standard library modÃ¼llerini tespit et"""
        
        # Python 3.x standard library modÃ¼llerinin bir listesi
        stdlib_modules = {
            'abc', 'argparse', 'array', 'ast', 'asyncio', 'base64', 'bisect',
            'calendar', 'collections', 'copy', 'csv', 'datetime', 'decimal',
            'functools', 'glob', 'gzip', 'hashlib', 'heapq', 'html', 'http',
            'importlib', 'io', 'itertools', 'json', 'logging', 'math', 'multiprocessing',
            'operator', 'os', 'pathlib', 'pickle', 'random', 're', 'shutil',
            'socket', 'sqlite3', 'string', 'subprocess', 'sys', 'tempfile',
            'threading', 'time', 'traceback', 'typing', 'urllib', 'uuid',
            'warnings', 'weakref', 'xml', 'zipfile', 'enum', 'dataclasses',
            'contextlib', 'concurrent', 'email', 'encodings'
        }
        
        return stdlib_modules
    
    def analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ğŸ“„ Tek dosyayÄ± analiz et"""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # AST parse
            tree = ast.parse(content, filename=str(file_path))
            
            # Import analyzer
            analyzer = ImportAnalyzer()
            analyzer.visit(tree)
            
            # SonuÃ§larÄ± organize et
            file_analysis = {
                "file_path": str(file_path),
                "relative_path": str(file_path.relative_to(self.project_root)),
                "imports": list(analyzer.imports),
                "from_imports": list(analyzer.from_imports),
                "local_imports": list(analyzer.local_imports),
                "import_details": analyzer.import_details,
                "total_imports": len(analyzer.imports) + len(analyzer.from_imports),
                "external_dependencies": [],
                "local_dependencies": []
            }
            
            # External vs local classification
            all_imported_modules = analyzer.imports | analyzer.from_imports
            
            for module in all_imported_modules:
                base_module = module.split('.')[0]
                
                if base_module in self.builtin_modules or base_module in self.standard_library:
                    # Built-in or standard library
                    continue
                elif base_module in self.project_modules or module.startswith('.'):
                    # Local project module
                    file_analysis["local_dependencies"].append(module)
                    self.local_modules.add(module)
                else:
                    # External package
                    file_analysis["external_dependencies"].append(base_module)
                    self.external_packages.add(base_module)
            
            return file_analysis
            
        except SyntaxError as e:
            logger.warning(f"âš ï¸ Syntax error in {file_path}: {e}")
            return {"file_path": str(file_path), "error": "syntax_error", "details": str(e)}
        
        except Exception as e:
            logger.error(f"âŒ Error analyzing {file_path}: {e}")
            return {"file_path": str(file_path), "error": "analysis_error", "details": str(e)}
    
    def analyze_project(self) -> Dict[str, Any]:
        """ğŸ” TÃ¼m projeyi analiz et"""
        
        logger.info("ğŸ” Proje analizi baÅŸlatÄ±lÄ±yor...")
        
        # TÃ¼m Python dosyalarÄ±nÄ± bul
        python_files = []
        
        # Ana klasÃ¶rler
        search_paths = [
            self.project_root,
            self.project_root / "utils",
            self.project_root / "strategies", 
            self.project_root / "optimization",
            self.project_root / "scripts"
        ]
        
        for search_path in search_paths:
            if search_path.exists():
                python_files.extend(search_path.glob("*.py"))
                # Alt klasÃ¶rlerde de ara
                python_files.extend(search_path.glob("*/*.py"))
        
        # Duplike dosyalarÄ± kaldÄ±r
        python_files = list(set(python_files))
        
        logger.info(f"ğŸ“Š {len(python_files)} Python dosyasÄ± bulundu")
        
        # Her dosyayÄ± analiz et
        analysis_results = {}
        successful_analyses = 0
        
        for file_path in python_files:
            try:
                file_analysis = self.analyze_file(file_path)
                relative_path = str(file_path.relative_to(self.project_root))
                analysis_results[relative_path] = file_analysis
                
                if "error" not in file_analysis:
                    successful_analyses += 1
                    
                    # Dependency graph'e ekle
                    self._add_to_dependency_graph(file_analysis)
                
            except Exception as e:
                logger.error(f"âŒ {file_path} analiz hatasÄ±: {e}")
        
        logger.info(f"âœ… {successful_analyses}/{len(python_files)} dosya baÅŸarÄ±yla analiz edildi")
        
        # Analiz Ã¶zetini oluÅŸtur
        project_analysis = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "project_root": str(self.project_root.absolute()),
            "total_files": len(python_files),
            "successful_analyses": successful_analyses,
            "file_analyses": analysis_results,
            "summary": self._generate_analysis_summary(),
            "dependency_graph_info": self._analyze_dependency_graph()
        }
        
        self.file_dependencies = analysis_results
        
        return project_analysis
    
    def _add_to_dependency_graph(self, file_analysis: Dict[str, Any]) -> None:
        """ğŸ“ˆ BaÄŸÄ±mlÄ±lÄ±k grafiÄŸine dosya ekle"""
        
        file_path = file_analysis["relative_path"]
        
        # Node'u ekle
        self.dependency_graph.add_node(file_path)
        
        # Local dependencies iÃ§in edge'leri ekle
        for dep in file_analysis.get("local_dependencies", []):
            # Relative import'larÄ± dÃ¼zelt
            if dep.startswith('.'):
                # TODO: Relative import resolution
                continue
            
            # Module path'i dosya path'ine Ã§evir
            potential_paths = [
                f"{dep.replace('.', '/')}.py",
                f"{dep.replace('.', '/')}/{dep.split('.')[-1]}.py",
                f"{dep}/{dep.split('.')[-1]}.py"
            ]
            
            for pot_path in potential_paths:
                if pot_path in self.file_dependencies:
                    self.dependency_graph.add_edge(file_path, pot_path)
                    break
    
    def _generate_analysis_summary(self) -> Dict[str, Any]:
        """ğŸ“Š Analiz Ã¶zetini oluÅŸtur"""
        
        # External packages'i topla
        all_external = set()
        all_local = set()
        total_imports = 0
        
        for file_analysis in self.file_dependencies.values():
            if "error" not in file_analysis:
                all_external.update(file_analysis.get("external_dependencies", []))
                all_local.update(file_analysis.get("local_dependencies", []))
                total_imports += file_analysis.get("total_imports", 0)
        
        return {
            "total_external_packages": len(all_external),
            "external_packages": sorted(list(all_external)),
            "total_local_modules": len(all_local),
            "local_modules": sorted(list(all_local)),
            "total_imports": total_imports,
            "most_used_externals": self._get_most_used_packages(all_external),
            "dependency_statistics": self._calculate_dependency_stats()
        }
    
    def _get_most_used_packages(self, external_packages: Set[str]) -> List[Tuple[str, int]]:
        """ğŸ“ˆ En Ã§ok kullanÄ±lan paketleri bul"""
        
        package_counts = defaultdict(int)
        
        for file_analysis in self.file_dependencies.values():
            if "error" not in file_analysis:
                for pkg in file_analysis.get("external_dependencies", []):
                    package_counts[pkg] += 1
        
        # SÄ±rala ve dÃ¶ndÃ¼r
        return sorted(package_counts.items(), key=lambda x: x[1], reverse=True)
    
    def _calculate_dependency_stats(self) -> Dict[str, Any]:
        """ğŸ“Š BaÄŸÄ±mlÄ±lÄ±k istatistiklerini hesapla"""
        
        external_counts = []
        local_counts = []
        
        for file_analysis in self.file_dependencies.values():
            if "error" not in file_analysis:
                external_counts.append(len(file_analysis.get("external_dependencies", [])))
                local_counts.append(len(file_analysis.get("local_dependencies", [])))
        
        if not external_counts:
            return {"error": "No valid files to analyze"}
        
        return {
            "avg_external_deps_per_file": sum(external_counts) / len(external_counts),
            "max_external_deps": max(external_counts),
            "avg_local_deps_per_file": sum(local_counts) / len(local_counts),
            "max_local_deps": max(local_counts),
            "files_with_most_external_deps": self._find_files_with_most_deps("external"),
            "files_with_most_local_deps": self._find_files_with_most_deps("local")
        }
    
    def _find_files_with_most_deps(self, dep_type: str) -> List[Tuple[str, int]]:
        """ğŸ“‹ En Ã§ok baÄŸÄ±mlÄ±lÄ±ÄŸa sahip dosyalarÄ± bul"""
        
        file_dep_counts = []
        dep_key = f"{dep_type}_dependencies"
        
        for relative_path, file_analysis in self.file_dependencies.items():
            if "error" not in file_analysis:
                dep_count = len(file_analysis.get(dep_key, []))
                file_dep_counts.append((relative_path, dep_count))
        
        return sorted(file_dep_counts, key=lambda x: x[1], reverse=True)[:10]
    
    def _analyze_dependency_graph(self) -> Dict[str, Any]:
        """ğŸ“ˆ BaÄŸÄ±mlÄ±lÄ±k grafiÄŸi analizini yap"""
        
        if not self.dependency_graph.nodes():
            return {"error": "Empty dependency graph"}
        
        try:
            # Temel graph metrics
            metrics = {
                "node_count": self.dependency_graph.number_of_nodes(),
                "edge_count": self.dependency_graph.number_of_edges(),
                "is_directed": self.dependency_graph.is_directed(),
                "density": nx.density(self.dependency_graph)
            }
            
            # DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼
            try:
                cycles = list(nx.simple_cycles(self.dependency_graph))
                metrics["circular_dependencies"] = cycles
                metrics["has_circular_deps"] = len(cycles) > 0
                metrics["circular_dependency_count"] = len(cycles)
            except Exception as e:
                metrics["circular_dependency_error"] = str(e)
            
            # En Ã§ok baÄŸÄ±mlÄ±lÄ±ÄŸa sahip dosyalar
            in_degrees = dict(self.dependency_graph.in_degree())
            out_degrees = dict(self.dependency_graph.out_degree())
            
            metrics["most_dependent_files"] = sorted(
                in_degrees.items(), key=lambda x: x[1], reverse=True
            )[:10]
            
            metrics["most_depended_upon"] = sorted(
                out_degrees.items(), key=lambda x: x[1], reverse=True  
            )[:10]
            
            return metrics
            
        except Exception as e:
            return {"error": f"Graph analysis failed: {e}"}
    
    def generate_requirements_txt(self, include_versions: bool = True) -> str:
        """ğŸ“‹ requirements.txt oluÅŸtur"""
        
        logger.info("ğŸ“‹ requirements.txt oluÅŸturuluyor...")
        
        if not self.external_packages:
            logger.warning("âš ï¸ External package bulunamadÄ±")
            return "# No external dependencies found\n"
        
        requirements_lines = []
        requirements_lines.append("# Generated by Phoenix Dependency Analyzer")
        requirements_lines.append(f"# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        requirements_lines.append(f"# Total packages: {len(self.external_packages)}")
        requirements_lines.append("")
        
        # Paketleri kategorilendir
        categorized_packages = self._categorize_packages()
        
        for category, packages in categorized_packages.items():
            if packages:
                requirements_lines.append(f"# {category}")
                
                for package in sorted(packages):
                    if include_versions:
                        version = self._get_package_version(package)
                        if version:
                            requirements_lines.append(f"{package}=={version}")
                        else:
                            requirements_lines.append(f"{package}  # Version not detected")
                    else:
                        requirements_lines.append(package)
                
                requirements_lines.append("")
        
        return '\n'.join(requirements_lines)
    
    def _categorize_packages(self) -> Dict[str, List[str]]:
        """ğŸ“¦ Paketleri kategorilere ayÄ±r"""
        
        categories = {
            "Core Data Science": [],
            "Machine Learning": [],
            "Trading & Finance": [],
            "Async & Networking": [],
            "Optimization": [],
            "Visualization": [],
            "Testing & Development": [],
            "Other": []
        }
        
        # Kategori mapping'i
        category_mapping = {
            "Core Data Science": ["pandas", "numpy", "scipy", "scikit-learn", "sklearn"],
            "Machine Learning": ["torch", "pytorch", "tensorflow", "keras", "xgboost", "lightgbm", "catboost"],
            "Trading & Finance": ["ccxt", "pandas_ta", "ta", "backtrader", "zipline", "quantlib"],
            "Async & Networking": ["aiohttp", "asyncio", "requests", "websockets"],
            "Optimization": ["optuna", "hyperopt", "skopt", "scipy.optimize"],
            "Visualization": ["matplotlib", "seaborn", "plotly", "bokeh"],
            "Testing & Development": ["pytest", "unittest", "mock", "coverage", "black", "flake8"]
        }
        
        # Paketleri kategorilere ata
        categorized = set()
        
        for category, keywords in category_mapping.items():
            for package in self.external_packages:
                if any(keyword in package.lower() for keyword in keywords):
                    categories[category].append(package)
                    categorized.add(package)
        
        # Kategorize edilmeyen paketleri "Other"a ekle
        for package in self.external_packages:
            if package not in categorized:
                categories["Other"].append(package)
        
        return categories
    
    def _get_package_version(self, package_name: str) -> Optional[str]:
        """ğŸ“¦ Paketin yÃ¼klÃ¼ versiyonunu al"""
        
        try:
            # pip show ile versiyon bilgisi al
            result = subprocess.run(
                ["pip", "show", package_name],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.split('\n'):
                    if line.startswith('Version:'):
                        return line.split(':', 1)[1].strip()
            
            return None
            
        except Exception:
            return None
    
    def check_circular_dependencies(self) -> Dict[str, Any]:
        """ğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼"""
        
        logger.info("ğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼...")
        
        if not self.dependency_graph.nodes():
            return {"error": "Dependency graph is empty"}
        
        try:
            cycles = list(nx.simple_cycles(self.dependency_graph))
            
            circular_analysis = {
                "has_circular_dependencies": len(cycles) > 0,
                "circular_dependency_count": len(cycles),
                "cycles": cycles,
                "severity": "high" if len(cycles) > 0 else "none",
                "recommendations": []
            }
            
            if cycles:
                logger.warning(f"âš ï¸ {len(cycles)} dÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k tespit edildi!")
                
                for i, cycle in enumerate(cycles):
                    logger.warning(f"   Cycle {i+1}: {' -> '.join(cycle)} -> {cycle[0]}")
                    
                    circular_analysis["recommendations"].append({
                        "cycle": cycle,
                        "suggestion": f"Break cycle by refactoring common functionality into a separate module"
                    })
            else:
                logger.info("âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k bulunamadÄ±")
            
            return circular_analysis
            
        except Exception as e:
            logger.error(f"âŒ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼ hatasÄ±: {e}")
            return {"error": str(e)}
    
    def optimize_imports(self) -> Dict[str, Any]:
        """âš¡ Import optimizasyonu Ã¶nerileri"""
        
        logger.info("âš¡ Import optimizasyonu analizi...")
        
        optimization_results = {
            "unused_imports": [],
            "duplicate_imports": [],
            "optimization_suggestions": [],
            "total_optimizable_files": 0
        }
        
        for relative_path, file_analysis in self.file_dependencies.items():
            if "error" not in file_analysis:
                file_optimizations = self._analyze_file_imports(file_analysis)
                
                if file_optimizations["has_optimizations"]:
                    optimization_results["total_optimizable_files"] += 1
                    
                    optimization_results["optimization_suggestions"].append({
                        "file": relative_path,
                        "optimizations": file_optimizations
                    })
        
        return optimization_results
    
    def _analyze_file_imports(self, file_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ğŸ“„ Dosya import analizi"""
        
        optimizations = {
            "has_optimizations": False,
            "unused_imports": [],
            "duplicate_imports": [],
            "suggestions": []
        }
        
        # Import details'leri analiz et
        import_details = file_analysis.get("import_details", [])
        
        # Duplicate import detection (basit)
        seen_imports = set()
        for imp in import_details:
            import_key = f"{imp['type']}:{imp['module']}"
            if import_key in seen_imports:
                optimizations["duplicate_imports"].append(imp)
                optimizations["has_optimizations"] = True
            seen_imports.add(import_key)
        
        return optimizations
    
    def save_analysis_results(self, analysis_results: Dict[str, Any]) -> None:
        """ğŸ’¾ Analiz sonuÃ§larÄ±nÄ± kaydet"""
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON raporu
        json_file = self.project_root / "logs" / f"dependency_analysis_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ Analiz sonuÃ§larÄ± kaydedildi: {json_file}")
        
        # Requirements.txt
        requirements_content = self.generate_requirements_txt()
        requirements_file = self.project_root / "requirements.txt"
        
        # Backup oluÅŸtur
        if requirements_file.exists():
            backup_file = self.project_root / f"requirements.txt.backup_{timestamp}"
            requirements_file.rename(backup_file)
            logger.info(f"ğŸ’¾ Eski requirements.txt backup: {backup_file}")
        
        with open(requirements_file, 'w', encoding='utf-8') as f:
            f.write(requirements_content)
        
        logger.info(f"âœ… Yeni requirements.txt oluÅŸturuldu: {requirements_file}")
        
        # Dependency graph (GraphML format)
        if self.dependency_graph.nodes():
            graph_file = self.project_root / "logs" / f"dependency_graph_{timestamp}.graphml"
            nx.write_graphml(self.dependency_graph, graph_file)
            logger.info(f"ğŸ“ˆ Dependency graph kaydedildi: {graph_file}")


def main():
    """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu"""
    
    parser = argparse.ArgumentParser(
        description="Phoenix Dependency Analyzer - Comprehensive Project Analysis",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
KullanÄ±m Ã–rnekleri:
  python dependency_analyzer.py --analyze-all                    # Tam analiz
  python dependency_analyzer.py --generate-requirements          # Requirements.txt oluÅŸtur
  python dependency_analyzer.py --check-circular                 # DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼
  python dependency_analyzer.py --optimize-imports               # Import optimizasyonu
  python dependency_analyzer.py --full-analysis                  # KapsamlÄ± analiz
        """
    )
    
    parser.add_argument('--analyze-all', action='store_true', help='TÃ¼m projeyi analiz et')
    parser.add_argument('--generate-requirements', action='store_true', help='Requirements.txt oluÅŸtur')
    parser.add_argument('--check-circular', action='store_true', help='DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼')
    parser.add_argument('--optimize-imports', action='store_true', help='Import optimizasyonu')
    parser.add_argument('--full-analysis', action='store_true', help='KapsamlÄ± analiz (hepsi)')
    parser.add_argument('--include-versions', action='store_true', default=True, help='Requirements.txt\'ye versiyon bilgisi ekle')
    parser.add_argument('--project-root', default='.', help='Proje kÃ¶k klasÃ¶rÃ¼')
    
    args = parser.parse_args()
    
    if not any([args.analyze_all, args.generate_requirements, args.check_circular, 
                args.optimize_imports, args.full_analysis]):
        parser.print_help()
        return
    
    # Analyzer'Ä± baÅŸlat
    analyzer = DependencyAnalyzer(project_root=args.project_root)
    
    try:
        print("ğŸ” PHOENIX DEPENDENCY ANALYZER")
        print("="*80)
        
        if args.full_analysis or args.analyze_all:
            print("ğŸ“Š Proje analizi baÅŸlatÄ±lÄ±yor...")
            analysis_results = analyzer.analyze_project()
            
            print("\nğŸ“‹ ANALÄ°Z SONUÃ‡LARI:")
            print(f"   ğŸ“„ Toplam dosya: {analysis_results['total_files']}")
            print(f"   âœ… BaÅŸarÄ±lÄ± analiz: {analysis_results['successful_analyses']}")
            
            summary = analysis_results['summary']
            print(f"   ğŸ“¦ External packages: {summary['total_external_packages']}")
            print(f"   ğŸ  Local modules: {summary['total_local_modules']}")
            print(f"   ğŸ“¥ Toplam import: {summary['total_imports']}")
            
            if summary.get('most_used_externals'):
                print(f"\nğŸ“ˆ EN Ã‡OK KULLANILAN PAKETLER:")
                for pkg, count in summary['most_used_externals'][:10]:
                    print(f"   ğŸ“¦ {pkg}: {count} dosyada kullanÄ±lÄ±yor")
            
            # SonuÃ§larÄ± kaydet
            analyzer.save_analysis_results(analysis_results)
        
        if args.full_analysis or args.generate_requirements:
            print("\nğŸ“‹ Requirements.txt oluÅŸturuluyor...")
            
            if not analyzer.external_packages:
                # EÄŸer analiz henÃ¼z yapÄ±lmamÄ±ÅŸsa
                analyzer.analyze_project()
            
            requirements_content = analyzer.generate_requirements_txt(args.include_versions)
            
            print("âœ… Requirements.txt oluÅŸturuldu!")
            print(f"   ğŸ“¦ {len(analyzer.external_packages)} external package")
            
            # Kategorilere gÃ¶re Ã¶zet
            categorized = analyzer._categorize_packages()
            for category, packages in categorized.items():
                if packages:
                    print(f"   ğŸ“‚ {category}: {len(packages)} package")
        
        if args.full_analysis or args.check_circular:
            print("\nğŸ”„ DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k kontrolÃ¼...")
            
            if not analyzer.dependency_graph.nodes():
                analyzer.analyze_project()
            
            circular_analysis = analyzer.check_circular_dependencies()
            
            if circular_analysis.get("has_circular_dependencies"):
                print(f"âš ï¸ {circular_analysis['circular_dependency_count']} dÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k bulundu!")
                
                for i, cycle in enumerate(circular_analysis.get("cycles", [])[:5]):  # Ä°lk 5'ini gÃ¶ster
                    print(f"   ğŸ”„ Cycle {i+1}: {' -> '.join(cycle)}")
            else:
                print("âœ… DÃ¶ngÃ¼sel baÄŸÄ±mlÄ±lÄ±k bulunamadÄ±")
        
        if args.full_analysis or args.optimize_imports:
            print("\nâš¡ Import optimizasyonu analizi...")
            
            if not analyzer.file_dependencies:
                analyzer.analyze_project()
            
            optimization_results = analyzer.optimize_imports()
            
            print(f"ğŸ“Š {optimization_results['total_optimizable_files']} dosya optimize edilebilir")
            
            if optimization_results['optimization_suggestions']:
                print("ğŸ’¡ Optimizasyon Ã¶nerileri:")
                for suggestion in optimization_results['optimization_suggestions'][:5]:  # Ä°lk 5'ini gÃ¶ster
                    print(f"   ğŸ“„ {suggestion['file']}")
                    if suggestion['optimizations']['duplicate_imports']:
                        print(f"      ğŸ”„ {len(suggestion['optimizations']['duplicate_imports'])} duplicate import")
        
        print("\nğŸ‰ ANALÄ°Z TAMAMLANDI!")
        print("ğŸ“Š DetaylÄ± sonuÃ§lar logs/ klasÃ¶rÃ¼nde")
        
    except Exception as e:
        logger.error(f"âŒ Analiz hatasÄ±: {e}")
        print(f"\nâŒ HATA: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()